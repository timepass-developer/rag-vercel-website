import { RequiredDataFromCollectionSlug } from 'payload'
import type { PostArgs } from './post-1'

export const post3: (args: PostArgs) => RequiredDataFromCollectionSlug<'posts'> = ({
  heroImage,
  blockImage,
  author,
}) => {
  return {
    slug: 'rag-technical-deep-dive',
    _status: 'published',
    authors: [author],
    content: {
      root: {
        type: 'root',
        children: [
          {
            type: 'block',
            fields: {
              blockName: 'Disclaimer',
              blockType: 'banner',
              content: {
                root: {
                  type: 'root',
                  children: [
                    {
                      type: 'paragraph',
                      children: [
                        {
                          type: 'text',
                          detail: 0,
                          format: 1,
                          mode: 'normal',
                          style: '',
                          text: 'Disclaimer: ',
                          version: 1,
                        },
                        {
                          type: 'text',
                          detail: 0,
                          format: 0,
                          mode: 'normal',
                          style: '',
                          text: 'This article explores RAG architecture in depth. To build your own RAG application without code, ',
                          version: 1,
                        },
                        {
                          type: 'link',
                          children: [
                            {
                              type: 'text',
                              detail: 0,
                              format: 0,
                              mode: 'normal',
                              style: '',
                              text: 'try our visual RAG Builder',
                              version: 1,
                            },
                          ],
                          direction: 'ltr',
                          fields: {
                            linkType: 'custom',
                            newTab: true,
                            url: '/rag-admin',
                          },
                          format: '',
                          indent: 0,
                          version: 3,
                        },
                        {
                          type: 'text',
                          detail: 0,
                          format: 0,
                          mode: 'normal',
                          style: '',
                          text: '.',
                          version: 1,
                        },
                      ],
                      direction: 'ltr',
                      format: '',
                      indent: 0,
                      textFormat: 1,
                      version: 1,
                    },
                  ],
                  direction: 'ltr',
                  format: '',
                  indent: 0,
                  version: 1,
                },
              },
              style: 'info',
            },
            format: '',
            version: 2,
          },
          {
            type: 'heading',
            children: [
              {
                type: 'text',
                detail: 0,
                format: 0,
                mode: 'normal',
                style: '',
                text: "Vector embeddings aren't just mathematical representations; ",
                version: 1,
              },
              {
                type: 'text',
                detail: 0,
                format: 2,
                mode: 'normal',
                style: '',
                text: "they're the language of semantic understanding. ",
                version: 1,
              },
              {
                type: 'text',
                detail: 0,
                format: 0,
                mode: 'normal',
                style: '',
                text: 'Explore how they form the foundation of advanced RAG systems that bridge content and context.',
                version: 1,
              },
            ],
            direction: 'ltr',
            format: '',
            indent: 0,
            tag: 'h2',
            version: 1,
          },
          {
            type: 'paragraph',
            children: [
              {
                type: 'text',
                detail: 0,
                format: 0,
                mode: 'normal',
                style: '',
                text: "At the core of every effective RAG system is the vector database - a specialized storage solution optimized for handling high-dimensional numerical representations of text. Unlike traditional databases that excel at exact matches and structured queries, vector databases specialize in similarity searches, finding content that's conceptually related even when the specific wording differs. This capability enables RAG systems to retrieve contextually relevant information based on the semantic meaning of a user's question rather than simple keyword matching. The sophisticated mathematical operations happening behind the scenes allow for nuanced understanding of language that approaches human comprehension in many domains.",
                version: 1,
              },
            ],
            direction: 'ltr',
            format: '',
            indent: 0,
            textFormat: 0,
            version: 1,
          },
          {
            type: 'block',
            fields: {
              blockName: '',
              blockType: 'mediaBlock',
              media: blockImage.id,
            },
            format: '',
            version: 2,
          },
          {
            type: 'heading',
            children: [
              {
                type: 'text',
                detail: 0,
                format: 0,
                mode: 'normal',
                style: '',
                text: 'The Four Pillars of RAG Architecture',
                version: 1,
              },
            ],
            direction: 'ltr',
            format: '',
            indent: 0,
            tag: 'h2',
            version: 1,
          },
          {
            type: 'paragraph',
            children: [
              {
                type: 'text',
                detail: 0,
                format: 0,
                mode: 'normal',
                style: '',
                text: 'A robust RAG system architecture consists of four essential components that work in concert to deliver accurate, contextually relevant responses:',
                version: 1,
              },
            ],
            direction: 'ltr',
            format: '',
            indent: 0,
            textFormat: 0,
            version: 1,
          },
          {
            type: 'heading',
            children: [
              {
                type: 'text',
                detail: 0,
                format: 0,
                mode: 'normal',
                style: '',
                text: '1. Document Processing Pipeline',
                version: 1,
              },
            ],
            direction: 'ltr',
            format: '',
            indent: 0,
            tag: 'h3',
            version: 1,
          },
          {
            type: 'paragraph',
            children: [
              {
                type: 'text',
                detail: 0,
                format: 0,
                mode: 'normal',
                style: '',
                text: "The foundation of any RAG system is its ability to ingest and process documents from various sources. This pipeline typically includes parsing different file formats (PDF, DOCX, HTML, etc.), extracting text content, and cleaning the data. Documents are then segmented into smaller chunks - typically paragraphs or semantically coherent sections - to optimize retrieval granularity. The ideal chunk size balances specificity (smaller chunks) with contextual completeness (larger chunks), usually ranging from 256 to 1000 tokens depending on the application's needs.",
                version: 1,
              },
            ],
            direction: 'ltr',
            format: '',
            indent: 0,
            textFormat: 0,
            version: 1,
          },
          {
            type: 'heading',
            children: [
              {
                type: 'text',
                detail: 0,
                format: 0,
                mode: 'normal',
                style: '',
                text: '2. Vector Embedding Generation',
                version: 1,
              },
            ],
            direction: 'ltr',
            format: '',
            indent: 0,
            tag: 'h3',
            version: 1,
          },
          {
            type: 'paragraph',
            children: [
              {
                type: 'text',
                detail: 0,
                format: 0,
                mode: 'normal',
                style: '',
                text: "Once documents are chunked, each segment is transformed into a vector embedding - a high-dimensional numerical representation that captures the semantic meaning of the text. These embeddings are generated using specialized models like OpenAI's text-embedding-ada-002 or open-source alternatives such as BERT or Sentence Transformers. The quality of these embeddings directly impacts retrieval accuracy, with more advanced models capturing subtle nuances of meaning and context. Each chunk's vector typically contains hundreds or thousands of dimensions, creating a rich mathematical representation of the text's semantic content.",
                version: 1,
              },
            ],
            direction: 'ltr',
            format: '',
            indent: 0,
            textFormat: 0,
            version: 1,
          },
          {
            type: 'heading',
            children: [
              {
                type: 'text',
                detail: 0,
                format: 0,
                mode: 'normal',
                style: '',
                text: '3. Vector Storage and Retrieval',
                version: 1,
              },
            ],
            direction: 'ltr',
            format: '',
            indent: 0,
            tag: 'h3',
            version: 1,
          },
          {
            type: 'paragraph',
            children: [
              {
                type: 'text',
                detail: 0,
                format: 0,
                mode: 'normal',
                style: '',
                text: "These vector embeddings are then stored in specialized vector databases optimized for similarity searches. When a user query arrives, it too is converted into a vector embedding using the same embedding model. The system then performs a similarity search (typically using cosine similarity or Euclidean distance) to find the document chunks whose vectors are mathematically closest to the query vector. This powerful approach allows the system to identify semantically relevant content even when there's little lexical overlap between the query and the stored documents.",
                version: 1,
              },
            ],
            direction: 'ltr',
            format: '',
            indent: 0,
            textFormat: 0,
            version: 1,
          },
          {
            type: 'heading',
            children: [
              {
                type: 'text',
                detail: 0,
                format: 0,
                mode: 'normal',
                style: '',
                text: '4. Response Generation',
                version: 1,
              },
            ],
            direction: 'ltr',
            format: '',
            indent: 0,
            tag: 'h3',
            version: 1,
          },
          {
            type: 'paragraph',
            children: [
              {
                type: 'text',
                detail: 0,
                format: 0,
                mode: 'normal',
                style: '',
                text: "The final component is the generation layer, where a large language model (LLM) synthesizes the retrieved information into a coherent response. Rather than generating an answer purely from its training data, the LLM is provided with the retrieved document chunks as context. A carefully crafted prompt instructs the model to use this context when answering the user's question, ensuring that responses are grounded in the retrieved information. This approach dramatically reduces hallucinations while preserving the LLM's fluent generation capabilities, resulting in responses that are both accurate and natural-sounding.",
                version: 1,
              },
            ],
            direction: 'ltr',
            format: '',
            indent: 0,
            textFormat: 0,
            version: 1,
          },
          {
            type: 'block',
            fields: {
              blockName: 'Build Without Coding',
              blockType: 'banner',
              content: {
                root: {
                  type: 'root',
                  children: [
                    {
                      type: 'paragraph',
                      children: [
                        {
                          type: 'text',
                          detail: 0,
                          format: 0,
                          mode: 'normal',
                          style: '',
                          text: 'Our RAG Builder platform abstracts away this technical complexity with an intuitive visual interface. Design your document processing pipeline, configure vector storage, and connect to AI models - all without writing code. Try it today to experience the power of RAG technology.',
                          version: 1,
                        },
                      ],
                      direction: 'ltr',
                      format: '',
                      indent: 0,
                      textFormat: 0,
                      version: 1,
                    },
                  ],
                  direction: 'ltr',
                  format: '',
                  indent: 0,
                  version: 1,
                },
              },
              style: 'success',
            },
            format: '',
            version: 2,
          },
          {
            type: 'heading',
            children: [
              {
                type: 'text',
                detail: 0,
                format: 0,
                mode: 'normal',
                style: '',
                text: 'Vector Search Implementation',
                version: 1,
              },
            ],
            direction: 'ltr',
            format: '',
            indent: 0,
            tag: 'h4',
            version: 1,
          },
          {
            type: 'code',
            language: 'typescript',
            code: '// Sample vector search implementation\ninterface Document {\n  id: string;\n  embedding: number[];\n  content: string;\n}\n\nfunction cosineSimilarity(vecA: number[], vecB: number[]): number {\n  let dotProduct = 0;\n  let normA = 0;\n  let normB = 0;\n  \n  for (let i = 0; i < vecA.length; i++) {\n    dotProduct += vecA[i] * vecB[i];\n    normA += vecA[i] * vecA[i];\n    normB += vecB[i] * vecB[i];\n  }\n  \n  return dotProduct / (Math.sqrt(normA) * Math.sqrt(normB));\n}\n\nfunction findSimilarDocuments(query: number[], documents: Document[], topK: number = 3): Document[] {\n  return documents\n    .map(doc => ({\n      ...doc,\n      score: cosineSimilarity(query, doc.embedding)\n    }))\n    .sort((a, b) => b.score - a.score)\n    .slice(0, topK);\n}',
            direction: 'ltr',
            format: '',
            indent: 0,
            version: 1,
          },
        ],
        direction: 'ltr',
        format: '',
        indent: 0,
        version: 1,
      },
    },
    heroImage: heroImage.id,
    meta: {
      description:
        'Explore the technical foundation of RAG systems: vector embeddings, document processing, and intelligent retrieval. A deep dive into how modern AI bridges content and context.',
      image: heroImage.id,
      title: 'RAG Architecture: A Technical Deep Dive',
    },
    relatedPosts: [],
    title: 'RAG Architecture: A Technical Deep Dive',
  }
}
